./datasets/MOSES
MOSES_bond: min = tensor(1) , max = tensor(4)
[*] Loaded dataset 'moses' from 'PyG-MOSES':
  Data(x=[37489760, 1], edge_index=[2, 80328666], edge_attr=[80328666], y=[1733214], idx=[1733214])
  undirected: True
  num graphs: 1733214
  avg num_nodes/graph: 21
  num node features: 1
  num edge features: 1
  num classes: (appears to be a regression task)
Precomputing Positional Encoding statistics: ['RRWP'] for all graphs...
  ...estimated to be undirected: True
Done! Took 01:13:11.50
Augmenting edges
Preprocessing PE
[*] Run ID 0: seed=0, split_index=0
    Starting now: 2024-08-11 04:33:51.184077
LatentDiffusion: Running in x0-prediction mode
DiffusionWrapper has 5.94 M params.
Restored from results/moses-unconditional_generation_encoder_structure_recon_dim_16-rebuttal/0/ckpt/39.ckpt with 0 missing and 0 unexpected keys
Using first stage also as cond stage.
LatentDiffusion(
  (model): DiffusionWrapper(
    (diffusion_model): DenoisingTransformer(
      (act): ReLU()
      (node_in_mlp): Sequential(
        (0): Linear(in_features=16, out_features=320, bias=True)
        (1): ReLU()
        (2): Linear(in_features=320, out_features=160, bias=True)
      )
      (edge_in_mlp): Sequential(
        (0): Linear(in_features=16, out_features=320, bias=True)
        (1): ReLU()
        (2): Linear(in_features=320, out_features=160, bias=True)
      )
      (cond_in_mlp): Sequential(
        (0): Linear(in_features=20, out_features=320, bias=True)
        (1): ReLU()
        (2): Linear(in_features=320, out_features=160, bias=True)
        (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (4): ReLU()
      )
      (cond_in_mlp_2): Sequential(
        (0): Linear(in_features=20, out_features=320, bias=True)
        (1): ReLU()
        (2): Linear(in_features=320, out_features=160, bias=True)
        (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (4): ReLU()
      )
      (cond_in_mlp_3): Sequential(
        (0): Linear(in_features=20, out_features=320, bias=True)
        (1): ReLU()
        (2): Linear(in_features=320, out_features=160, bias=True)
        (3): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (4): ReLU()
      )
      (cond_res_mlp): Sequential(
        (0): Linear(in_features=20, out_features=32, bias=True)
        (1): ReLU()
        (2): Linear(in_features=32, out_features=16, bias=True)
        (3): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
        (4): ReLU()
      )
      (batch_norm_in_h): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (batch_norm_in_e): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (temb_layer): Sequential(
        (0): Linear(in_features=128, out_features=256, bias=True)
        (1): ReLU()
        (2): Linear(in_features=256, out_features=128, bias=True)
      )
      (denoising_layers): ModuleList(
        (0-4): 5 x DenoisingTransformerLayer(in_channels=160, out_channels=160, heads=8, residual=True)
        [DenoisingTransformerLayer(
          (act): ReLU()
          (cross_attention): Cross_Attention(
            (dropout): Dropout(p=0.1, inplace=False)
            (Q): Linear(in_features=160, out_features=160, bias=True)
            (K): Linear(in_features=160, out_features=160, bias=False)
            (E1): Linear(in_features=160, out_features=160, bias=True)
            (E2): Linear(in_features=160, out_features=160, bias=True)
            (V): Linear(in_features=160, out_features=160, bias=False)
            (H): Linear(in_features=160, out_features=160, bias=True)
            (G1): Linear(in_features=160, out_features=160, bias=True)
            (G2): Linear(in_features=160, out_features=160, bias=True)
            (act): ReLU()
          )
          (attention): GTE_Attention(
            (dropout): Dropout(p=0.1, inplace=False)
            (Q): Linear(in_features=160, out_features=160, bias=True)
            (K): Linear(in_features=160, out_features=160, bias=False)
            (E): Linear(in_features=160, out_features=320, bias=True)
            (V): Linear(in_features=160, out_features=160, bias=False)
            (act): ReLU()
          )
          (temb_proj_h): Linear(in_features=128, out_features=160, bias=True)
          (temb_proj_e): Linear(in_features=128, out_features=160, bias=True)
          (O_h): Linear(in_features=160, out_features=160, bias=True)
          (O_e): Linear(in_features=160, out_features=160, bias=True)
          (O_h2): Linear(in_features=160, out_features=160, bias=True)
          (O_e2): Linear(in_features=160, out_features=160, bias=True)
          (FFN_h_layer1): Linear(in_features=160, out_features=320, bias=True)
          (FFN_h_layer2): Linear(in_features=320, out_features=160, bias=True)
          (FFN_h_layer3): Linear(in_features=160, out_features=320, bias=True)
          (FFN_h_layer4): Linear(in_features=320, out_features=160, bias=True)
          (batch_norm1_h): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (batch_norm1_e): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (batch_norm2_h): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (batch_norm2_e): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (batch_norm3_h): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (batch_norm3_e): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (batch_norm4_h): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (batch_norm4_e): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )]
      )
      (self_attn_layers): ModuleList(
        (0-4): 5 x GraphTransformerEncoderLayer(in_channels=160, out_channels=160, heads=8, residual=True)
        [GraphTransformerEncoderLayer(
          (act): ReLU()
          (attention): GTE_Attention(
            (dropout): Dropout(p=0.1, inplace=False)
            (Q): Linear(in_features=160, out_features=160, bias=True)
            (K): Linear(in_features=160, out_features=160, bias=False)
            (E): Linear(in_features=160, out_features=320, bias=True)
            (V): Linear(in_features=160, out_features=160, bias=False)
            (act): ReLU()
          )
          (temb_proj_h): Linear(in_features=128, out_features=160, bias=True)
          (temb_proj_e): Linear(in_features=128, out_features=160, bias=True)
          (O_h): Linear(in_features=160, out_features=160, bias=True)
          (O_e): Linear(in_features=160, out_features=160, bias=True)
          (batch_norm1_h): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (batch_norm1_e): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (FFN_h_layer1): Linear(in_features=160, out_features=320, bias=True)
          (FFN_h_layer2): Linear(in_features=320, out_features=160, bias=True)
          (FFN_e_layer1): Linear(in_features=160, out_features=320, bias=True)
          (FFN_e_layer2): Linear(in_features=320, out_features=160, bias=True)
          (batch_norm2_h): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (batch_norm2_e): BatchNorm1d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )]
      )
      (final_layer_node): Sequential(
        (0): Linear(in_features=160, out_features=32, bias=True)
        (1): ReLU()
        (2): Linear(in_features=32, out_features=16, bias=True)
      )
      (final_layer_edge): Sequential(
        (0): Linear(in_features=160, out_features=32, bias=True)
        (1): ReLU()
        (2): Linear(in_features=32, out_features=16, bias=True)
      )
      (final_norm_node_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (final_norm_edge_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (graph_out_mlp): Identity()
      (final_norm_node_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (graph_out_mlp_2): Identity()
      (final_norm_edge_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    )
  )
  (first_stage_model): GraphTransformerStructureEncoder(
    (act): ReLU()
    (node_emb): TypeDictNodeEncoder(
      (encoder): Embedding(8, 48, padding_idx=0)
    )
    (edge_emb): TypeDictEdgeEncoder(
      (encoder): Embedding(5, 48, padding_idx=0)
    )
    (prefix_emb): Embedding(1, 96, padding_idx=0)
    (label_embed_regression): Sequential(
      (0): Linear(in_features=1, out_features=192, bias=True)
      (1): ReLU()
      (2): Linear(in_features=192, out_features=96, bias=True)
      (3): Identity()
      (4): ReLU()
    )
    (label_embed_classification): Embedding(3, 96)
    (posenc_emb): Sequential(
      (0): Identity()
      (1): Linear(in_features=20, out_features=48, bias=True)
    )
    (posenc_emb_edge): Sequential(
      (0): Identity()
      (1): Linear(in_features=20, out_features=48, bias=True)
    )
    (node_in_mlp): Sequential(
      (0): Linear(in_features=96, out_features=192, bias=True)
      (1): ReLU()
      (2): Linear(in_features=192, out_features=96, bias=True)
    )
    (edge_in_mlp): Sequential(
      (0): Linear(in_features=96, out_features=192, bias=True)
      (1): ReLU()
      (2): Linear(in_features=192, out_features=96, bias=True)
    )
    (batch_norm_in_h): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (batch_norm_in_e): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (GTE_layers): ModuleList(
      (0-7): 8 x GraphTransformerEncoderLayer(in_channels=96, out_channels=96, heads=4, residual=True)
      [GraphTransformerEncoderLayer(
        (act): ReLU()
        (attention): GTE_Attention(
          (dropout): Dropout(p=0.5, inplace=False)
          (Q): Linear(in_features=96, out_features=96, bias=True)
          (K): Linear(in_features=96, out_features=96, bias=False)
          (E): Linear(in_features=96, out_features=192, bias=True)
          (V): Linear(in_features=96, out_features=96, bias=False)
          (act): ReLU()
        )
        (mpnn): GINE(
          (dropout): Dropout(p=0.2, inplace=False)
          (act): ReLU()
          (lin_edge): Linear(in_features=96, out_features=96, bias=True)
          (mlp): Sequential(
            (0): Linear(in_features=96, out_features=192, bias=True)
            (1): ReLU()
            (2): Linear(in_features=192, out_features=96, bias=True)
          )
        )
        (temb_proj_h): Linear(in_features=0, out_features=96, bias=True)
        (temb_proj_e): Linear(in_features=0, out_features=96, bias=True)
        (O_h): Linear(in_features=96, out_features=96, bias=True)
        (O_e): Linear(in_features=96, out_features=96, bias=True)
        (batch_norm1_h): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (batch_norm1_e): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (FFN_h_layer1): Linear(in_features=96, out_features=192, bias=True)
        (FFN_h_layer2): Linear(in_features=192, out_features=96, bias=True)
        (FFN_e_layer1): Linear(in_features=96, out_features=192, bias=True)
        (FFN_e_layer2): Linear(in_features=192, out_features=96, bias=True)
        (batch_norm2_h): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (batch_norm2_e): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )]
    )
    (final_layer_node): Sequential(
      (0): Linear(in_features=96, out_features=32, bias=True)
      (1): ReLU()
      (2): Linear(in_features=32, out_features=16, bias=True)
    )
    (final_layer_edge): Sequential(
      (0): Linear(in_features=96, out_features=32, bias=True)
      (1): ReLU()
      (2): Linear(in_features=32, out_features=16, bias=True)
    )
    (final_norm_node_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (final_norm_edge_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (graph_out_mlp): Identity()
    (final_norm_node_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (graph_out_mlp_2): Identity()
    (final_norm_edge_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (decode_node): Linear(in_features=16, out_features=8, bias=True)
    (decode_edge): Linear(in_features=16, out_features=5, bias=True)
    (decode_graph): Linear(in_features=16, out_features=1, bias=True)
    (decode_node_from_edge): Linear(in_features=16, out_features=8, bias=True)
    (decode_edge_from_node): Linear(in_features=16, out_features=5, bias=True)
  )
  (cond_stage_model): GraphTransformerStructureEncoder(
    (act): ReLU()
    (node_emb): TypeDictNodeEncoder(
      (encoder): Embedding(8, 48, padding_idx=0)
    )
    (edge_emb): TypeDictEdgeEncoder(
      (encoder): Embedding(5, 48, padding_idx=0)
    )
    (prefix_emb): Embedding(1, 96, padding_idx=0)
    (label_embed_regression): Sequential(
      (0): Linear(in_features=1, out_features=192, bias=True)
      (1): ReLU()
      (2): Linear(in_features=192, out_features=96, bias=True)
      (3): Identity()
      (4): ReLU()
    )
    (label_embed_classification): Embedding(3, 96)
    (posenc_emb): Sequential(
      (0): Identity()
      (1): Linear(in_features=20, out_features=48, bias=True)
    )
    (posenc_emb_edge): Sequential(
      (0): Identity()
      (1): Linear(in_features=20, out_features=48, bias=True)
    )
    (node_in_mlp): Sequential(
      (0): Linear(in_features=96, out_features=192, bias=True)
      (1): ReLU()
      (2): Linear(in_features=192, out_features=96, bias=True)
    )
    (edge_in_mlp): Sequential(
      (0): Linear(in_features=96, out_features=192, bias=True)
      (1): ReLU()
      (2): Linear(in_features=192, out_features=96, bias=True)
    )
    (batch_norm_in_h): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (batch_norm_in_e): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (GTE_layers): ModuleList(
      (0-7): 8 x GraphTransformerEncoderLayer(in_channels=96, out_channels=96, heads=4, residual=True)
      [GraphTransformerEncoderLayer(
        (act): ReLU()
        (attention): GTE_Attention(
          (dropout): Dropout(p=0.5, inplace=False)
          (Q): Linear(in_features=96, out_features=96, bias=True)
          (K): Linear(in_features=96, out_features=96, bias=False)
          (E): Linear(in_features=96, out_features=192, bias=True)
          (V): Linear(in_features=96, out_features=96, bias=False)
          (act): ReLU()
        )
        (mpnn): GINE(
          (dropout): Dropout(p=0.2, inplace=False)
          (act): ReLU()
          (lin_edge): Linear(in_features=96, out_features=96, bias=True)
          (mlp): Sequential(
            (0): Linear(in_features=96, out_features=192, bias=True)
            (1): ReLU()
            (2): Linear(in_features=192, out_features=96, bias=True)
          )
        )
        (temb_proj_h): Linear(in_features=0, out_features=96, bias=True)
        (temb_proj_e): Linear(in_features=0, out_features=96, bias=True)
        (O_h): Linear(in_features=96, out_features=96, bias=True)
        (O_e): Linear(in_features=96, out_features=96, bias=True)
        (batch_norm1_h): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (batch_norm1_e): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (FFN_h_layer1): Linear(in_features=96, out_features=192, bias=True)
        (FFN_h_layer2): Linear(in_features=192, out_features=96, bias=True)
        (FFN_e_layer1): Linear(in_features=96, out_features=192, bias=True)
        (FFN_e_layer2): Linear(in_features=192, out_features=96, bias=True)
        (batch_norm2_h): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (batch_norm2_e): BatchNorm1d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )]
    )
    (final_layer_node): Sequential(
      (0): Linear(in_features=96, out_features=32, bias=True)
      (1): ReLU()
      (2): Linear(in_features=32, out_features=16, bias=True)
    )
    (final_layer_edge): Sequential(
      (0): Linear(in_features=96, out_features=32, bias=True)
      (1): ReLU()
      (2): Linear(in_features=32, out_features=16, bias=True)
    )
    (final_norm_node_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (final_norm_edge_1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (graph_out_mlp): Identity()
    (final_norm_node_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (graph_out_mlp_2): Identity()
    (final_norm_edge_2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (decode_node): Linear(in_features=16, out_features=8, bias=True)
    (decode_edge): Linear(in_features=16, out_features=5, bias=True)
    (decode_graph): Linear(in_features=16, out_features=1, bias=True)
    (decode_node_from_edge): Linear(in_features=16, out_features=8, bias=True)
    (decode_edge_from_node): Linear(in_features=16, out_features=5, bias=True)
  )
)
accelerator: cuda:0
benchmark: False
bn:
  eps: 1e-05
  mom: 0.1
cfg_dest: config.yaml
cond:
  attn:
    O_e: True
    act: relu
    attn_dropout: 0.0
    batch_norm: True
    bn_momentum: 0.1
    bn_no_runner: False
    clamp: 5.0
    deg_scaler: False
    edge_enhance: True
    full_attn: True
    layer_norm: False
    norm_e: True
    sparse: False
    use: True
    use_bias: False
  batch_norm: True
  bn_momentum: 0.1
  bn_no_runner: False
  edge_encoder_name: TypeDictEdge
  layer_norm: False
  node_encoder_name: TypeDictNode
custom_metrics: []
dataset:
  add_virtual_node_edge: False
  cache_load: False
  cache_save: False
  dir: ./datasets
  edge_dim: 128
  edge_encoder: True
  edge_encoder_bn: False
  edge_encoder_name: TypeDictEdge
  edge_encoder_num_types: 5
  edge_message_ratio: 0.8
  edge_negative_sampling_ratio: 1.0
  edge_train_mode: all
  encoder: True
  encoder_bn: True
  encoder_dim: 128
  encoder_name: db
  format: PyG-MOSES
  label_column: none
  label_table: none
  location: local
  name: moses
  node_encoder: True
  node_encoder_bn: False
  node_encoder_name: TypeDictNode
  node_encoder_num_types: 8
  num_hop: 0
  remove_feature: False
  remove_h: False
  resample_disjoint: False
  resample_negative: False
  shuffle_split: True
  slic_compactness: 10
  split: [0.8, 0.1, 0.1]
  split_dir: ./splits
  split_index: 0
  split_mode: standard
  subgraph: False
  task: graph
  task_type: regression
  to_undirected: False
  transductive: False
  transform: none
  tu_simple: True
devices: 1
diffusion:
  cond_stage_config: __is_first_stage__
  cond_stage_key: pe
  conditioning_key: crossattn
  edge_factor: 1.0
  first_stage_config: results/moses-unconditional_generation_encoder_structure_recon_dim_16-rebuttal/0/ckpt/39.ckpt
  force_undirected: True
  graph_factor: 0.0
  hid_dim: 16
  node_factor: 1.0
  parameterization: x0
  recon_factor: 0.0
  task_factor: 0.0
dt:
  O_e: True
  act: relu
  attn:
    O_e: True
    act: relu
    attn_dropout: 0.0
    attn_product: mul
    attn_reweight: False
    batch_norm: True
    bn_momentum: 0.1
    bn_no_runner: False
    clamp: 5.0
    deg_scaler: False
    edge_enhance: True
    edge_reweight: False
    full_attn: True
    fwl: False
    layer_norm: False
    norm_e: True
    score_act: True
    signed_sqrt: True
    sparse: False
    use: True
    use_bias: False
  attn_dropout: 0.1
  batch_norm: True
  bn_momentum: 0.1
  bn_no_runner: False
  cond_dim: 20
  condition_list: ['masked_graph']
  dropout: 0.05
  ff_e: False
  ff_e_ca: False
  ff_e_sa: True
  final_norm: True
  force_undirected: True
  hid_dim: 160
  in_dim: 16
  layer_norm: False
  norm_e: True
  num_heads: 8
  num_layers: 5
  out_dim: 16
  pool: mean
  pool_edge: True
  pool_vn: False
  post_pool: False
  residual: True
  self_attn: True
  temb_dim: 128
  update_e: True
  use_time: True
encoder:
  O_e: True
  act: relu
  add_virtual_node_edge: False
  attn:
    O_e: True
    act: relu
    attn_dropout: 0.0
    attn_product: mul
    attn_reweight: False
    batch_norm: True
    bn_momentum: 0.1
    bn_no_runner: False
    clamp: 5.0
    deg_scaler: False
    edge_enhance: True
    edge_reweight: False
    full_attn: True
    fwl: False
    layer_norm: False
    norm_e: True
    score_act: True
    signed_sqrt: True
    sparse: False
    use: True
    use_bias: False
  attn_dropout: 0.5
  batch_norm: True
  bn_momentum: 0.1
  bn_no_runner: False
  dropout: 0.2
  edge_encoder: True
  edge_encoder_bn: False
  edge_encoder_name: TypeDictEdge
  edge_encoder_num_types: 5
  ff_e: True
  final_norm: True
  force_undirected: True
  hid_dim: 96
  in_dim: 48
  label_embed_type: add_all
  layer_norm: False
  model_type: GraphTransformerStructureEncoder
  mpnn:
    act: relu
    dropout: 0.2
    edge_enhance: True
    enable: True
    project_edge: True
  node_encoder: True
  node_encoder_bn: False
  node_encoder_name: TypeDictNode
  node_encoder_num_types: 8
  norm_e: True
  num_heads: 4
  num_layers: 8
  num_task: 1
  out_dim: 16
  pe_raw_norm: None
  pool: mean
  pool_edge: True
  pool_vn: False
  posenc_dim: 48
  posenc_in_dim: 20
  posenc_in_dim_edge: 20
  post_pool: False
  prefix_dim: 96
  prefix_type: add_all
  residual: True
  task_type: regression
  temb_dim: 0
  update_e: True
  use_time: False
example_arg: example
example_group:
  example_arg: example
gnn:
  act: relu
  agg: mean
  att_final_linear: False
  att_final_linear_bn: False
  att_heads: 1
  batchnorm: True
  clear_feature: True
  dim_inner: 64
  dropout: 0.0
  head: san_graph
  keep_edge: 0.5
  l2norm: True
  layer_type: generalconv
  layers_mp: 2
  layers_post_mp: 3
  layers_pre_mp: 0
  msg_direction: single
  normalize_adj: False
  residual: False
  self_msg: concat
  skip_every: 1
  stage_type: stack
gpu_mem: False
gt:
  attn:
    O_e: True
    act: relu
    clamp: 5.0
    deg_scaler: True
    edge_enhance: True
    full_attn: True
    fwl: False
    norm_e: True
    sparse: False
    use: True
    use_bias: False
  attn_dropout: 0.2
  batch_norm: True
  bigbird:
    add_cross_attention: False
    attention_type: block_sparse
    block_size: 3
    chunk_size_feed_forward: 0
    hidden_act: relu
    is_decoder: False
    layer_norm_eps: 1e-06
    max_position_embeddings: 128
    num_random_blocks: 3
    use_bias: False
  bn_momentum: 0.1
  bn_no_runner: False
  dim_hidden: 64
  dropout: 0.0
  full_graph: True
  gamma: 1e-05
  layer_norm: False
  layer_type: GritTransformer
  layers: 10
  n_heads: 8
  pna_degrees: []
  residual: True
  update_e: True
mem:
  inplace: False
metric_agg: argmin
metric_best: mae
mlflow:
  name: MOSES
  project: MOSES
  use: False
model:
  edge_decoding: dot
  graph_pooling: add
  loss_fun: l1
  match_upper: True
  size_average: mean
  thresh: 0.5
  type: LatentDiffusion
name_tag: 
num_threads: 6
num_workers: 0
optim:
  base_lr: 0.0005
  batch_accumulation: 1
  clip_grad_norm: True
  lr_decay: 0.1
  max_epoch: 1000
  min_lr: 1e-06
  momentum: 0.9
  num_warmup_epochs: 50
  optimizer: adamW
  reduce_factor: 0.1
  schedule_patience: 10
  scheduler: cosine_with_warmup
  steps: [30, 60, 90]
  weight_decay: 1e-05
out_dir: results/moses-unconditional_generation_diffusion_pe_simple-encoder_structure_recon_dim_16-rebuttal
posenc_ERE:
  accuracy: 0.1
  dim_pe: 16
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_ERN:
  accuracy: 0.1
  dim_pe: 16
  enable: False
  er_dim: none
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_EdgeRWSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_ElstaticSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: range(10)
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_EquivStableLapPE:
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  raw_norm_type: none
posenc_HKdiagSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_HodgeLap1PE:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_InterRWSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_LapPE:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_RD:
  dim_pe: 16
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_RRWP:
  add_identity: True
  dim_pe: 16
  enable: True
  ksteps: 20
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
  real_emb: True
  spd: False
posenc_RWSE:
  dim_pe: 16
  enable: False
  kernel:
    times: []
    times_func: 
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  post_layers: 0
  raw_norm_type: none
posenc_SignNet:
  dim_pe: 16
  eigen:
    eigvec_norm: L2
    laplacian_norm: sym
    max_freqs: 10
  enable: False
  layers: 3
  local: False
  model: none
  n_heads: 4
  pass_as_var: False
  phi_hidden_dim: 64
  phi_out_dim: 4
  post_layers: 0
  raw_norm_type: none
prep:
  add_edge_index: True
  add_reverse_edges: True
  add_self_loops: False
  dist_cutoff: 510
  dist_enable: False
  exp: False
  exp_algorithm: Random-d
  exp_count: 1
  exp_deg: 5
  exp_max_num_iters: 100
  layer_edge_indices_dir: None
  num_virt_node: 0
  train_percent: 0.6
  use_exp_edges: True
pretrained:
  dir: 
  freeze_main: False
  reset_prediction_head: True
print: both
round: 5
run_dir: results/moses-unconditional_generation_diffusion_pe_simple-encoder_structure_recon_dim_16-rebuttal/0
run_id: 0
run_multiple_splits: []
seed: 0
share:
  dim_in: 1
  dim_out: 1
  num_splits: 3
tensorboard_agg: True
tensorboard_each_run: False
train:
  auto_resume: False
  batch_size: 512
  ckpt_best: True
  ckpt_clean: False
  ckpt_period: 25
  enable_ckpt: True
  ensemble_mode: none
  ensemble_repeat: 1
  epoch_resume: -1
  eval_batch_size: 1024
  eval_period: 50
  iter_per_epoch: 32
  mode: qm9_unconditional
  neighbor_sizes: [20, 15, 10, 5]
  node_per_graph: 32
  pretrain:
    atom_bond_only: True
    edge_factor: 1.0
    graph_factor: 0.0
    input_target: False
    mask_edge_prob: 0.0
    mask_label_prob: 0.0
    mask_node_prob: 0.0
    node_factor: 1.0
    original_task: False
    recon: all
  radius: extend
  sample_node: False
  sampler: full_batch
  skip_train_eval: False
  start_eval_epoch: -1
  walk_length: 4
val:
  node_per_graph: 32
  radius: extend
  sample_node: False
  sampler: full_batch
view_emb: False
wandb:
  entity: gtransformers
  name: 
  project: MOSES
  use: False
Num parameters: 7543411
Start from epoch 0
Dataset smiles were found.
Dataset smiles were found.
Ground truth test FCD: -3.979039320256561e-13
Ground truth test NSPDK MMD: 0.0
Graph with 23 nodes and 25 edges
Graph with 22 nodes and 24 edges
Graph with 22 nodes and 24 edges
Graph with 24 nodes and 27 edges
Graph with 18 nodes and 18 edges
Graph with 25 nodes and 27 edges
Graph with 23 nodes and 25 edges
Graph with 20 nodes and 21 edges
Graph with 20 nodes and 21 edges
Graph with 22 nodes and 24 edges
Graph with 19 nodes and 20 edges
Graph with 23 nodes and 25 edges
Graph with 24 nodes and 26 edges
Graph with 26 nodes and 29 edges
Graph with 21 nodes and 23 edges
Graph with 24 nodes and 25 edges
Graph with 20 nodes and 21 edges
Graph with 22 nodes and 23 edges
Graph with 18 nodes and 20 edges
Graph with 20 nodes and 21 edges
Graph with 19 nodes and 20 edges
Graph with 23 nodes and 24 edges
Graph with 24 nodes and 26 edges
Graph with 23 nodes and 23 edges
Graph with 23 nodes and 24 edges
Graph with 21 nodes and 23 edges
Graph with 25 nodes and 27 edges
Graph with 20 nodes and 22 edges
Graph with 21 nodes and 23 edges
Graph with 25 nodes and 27 edges
Graph with 22 nodes and 24 edges
Graph with 24 nodes and 27 edges
Graph with 26 nodes and 29 edges
Graph with 25 nodes and 28 edges
Graph with 21 nodes and 22 edges
Graph with 17 nodes and 17 edges
Graph with 23 nodes and 24 edges
Graph with 22 nodes and 24 edges
Graph with 22 nodes and 23 edges
Graph with 21 nodes and 22 edges
Graph with 24 nodes and 26 edges
Graph with 25 nodes and 27 edges
Graph with 25 nodes and 28 edges
Graph with 23 nodes and 24 edges
Graph with 22 nodes and 24 edges
Graph with 21 nodes and 22 edges
Graph with 16 nodes and 16 edges
Graph with 20 nodes and 22 edges
Graph with 22 nodes and 23 edges
Graph with 22 nodes and 25 edges
train: {'epoch': 0, 'time_epoch': 3549.44213, 'eta': 3545892.68582, 'eta_hours': 984.97019, 'loss': 21.49839046, 'lr': 0.0, 'params': 7543411, 'time_iter': 1.28, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 16.673948643820115, 'loss_edge': 4.824441819256577}
...computing epoch stats took: 0.58s
tensor([[-1.6572e+00,  8.6861e-01, -2.1022e-01,  1.2010e+00,  1.5599e-01,
         -8.7232e-01, -1.2748e+00,  8.0318e-01, -1.0910e+00,  6.1861e-01,
          8.2041e-01, -3.9798e-01, -4.4551e-01,  9.2752e-01, -1.1962e+00,
          1.7499e+00],
        [-1.8048e+00,  8.9927e-01, -1.2930e-01,  1.1644e+00,  7.3851e-02,
         -6.4819e-01, -1.2353e+00,  7.2408e-01, -9.7010e-01,  5.4637e-01,
          8.1372e-01, -5.1152e-01, -5.5851e-01,  1.0228e+00, -1.1725e+00,
          1.7857e+00],
        [-1.6279e+00,  9.4671e-01, -1.3282e-01,  1.1816e+00,  8.9775e-02,
         -9.1426e-01, -1.2821e+00,  8.1790e-01, -1.0914e+00,  5.9765e-01,
          7.5041e-01, -3.9627e-01, -4.8346e-01,  9.0297e-01, -1.1557e+00,
          1.7969e+00],
        [-1.6449e+00,  1.0404e+00, -6.4802e-02,  1.1547e+00,  1.7485e-01,
         -9.5474e-01, -1.2780e+00,  7.9771e-01, -1.2340e+00,  2.4998e-01,
          9.3530e-01, -1.5417e-01, -3.2581e-01,  9.3366e-01, -1.2523e+00,
          1.6222e+00],
        [-1.5256e+00,  8.9898e-01, -3.3762e-01,  1.0756e+00,  3.1987e-01,
         -7.2923e-01, -1.1905e+00,  5.6369e-01, -9.4179e-01,  7.2435e-01,
          9.4606e-01, -5.4154e-01, -6.3989e-01,  8.8590e-01, -1.3833e+00,
          1.8750e+00],
        [-1.6637e+00,  9.4864e-01, -2.0376e-01,  1.1230e+00,  1.9903e-01,
         -8.3143e-01, -1.1991e+00,  6.7349e-01, -9.0478e-01,  6.5808e-01,
          9.5418e-01, -5.6081e-01, -5.3209e-01,  8.8903e-01, -1.3168e+00,
          1.7670e+00],
        [-1.6596e+00,  1.0855e+00, -3.3588e-01,  9.4182e-01,  1.7162e-01,
         -7.3862e-01, -1.1815e+00,  6.3781e-01, -8.9913e-01,  7.5386e-01,
          8.5176e-01, -5.1759e-01, -5.9559e-01,  1.0155e+00, -1.3178e+00,
          1.7878e+00],
        [-1.8327e+00,  1.0594e+00, -1.2661e-01,  1.0413e+00, -2.5613e-02,
         -6.7466e-01, -1.3254e+00,  6.9107e-01, -9.6967e-01,  5.5181e-01,
          8.3643e-01, -4.2540e-01, -3.9590e-01,  1.0435e+00, -1.1736e+00,
          1.7261e+00],
        [-1.6497e+00,  8.7207e-01, -3.6143e-01,  1.0274e+00,  2.2332e-01,
         -8.4475e-01, -1.1951e+00,  8.8446e-01, -1.1715e+00,  7.6135e-01,
          9.2941e-01, -3.8174e-01, -4.7282e-01,  9.5227e-01, -1.2312e+00,
          1.6580e+00],
        [-1.6509e+00,  9.8924e-01, -1.6417e-01,  1.0546e+00,  1.1194e-01,
         -8.3846e-01, -1.3332e+00,  7.7190e-01, -1.1095e+00,  5.4880e-01,
          8.0953e-01, -3.1404e-01, -3.5721e-01,  9.4631e-01, -1.2513e+00,
          1.7865e+00],
        [-1.5152e+00,  9.0247e-01,  4.6959e-02,  1.2186e+00,  2.6661e-01,
         -1.1146e+00, -1.2041e+00,  5.6062e-01, -1.1907e+00,  5.0844e-01,
          9.6441e-01, -5.0473e-01, -2.7586e-01,  8.1745e-01, -1.2541e+00,
          1.7737e+00],
        [-1.6300e+00,  9.4845e-01, -1.8907e-01,  1.0502e+00,  1.7930e-01,
         -8.6299e-01, -1.2601e+00,  6.9580e-01, -9.1183e-01,  6.8221e-01,
          7.0912e-01, -4.9598e-01, -3.9847e-01,  8.7525e-01, -1.3238e+00,
          1.9319e+00],
        [-1.5542e+00,  9.0028e-01, -4.2301e-04,  1.2836e+00,  1.8421e-01,
         -8.9579e-01, -1.4035e+00,  4.5437e-01, -1.0510e+00,  1.3128e-01,
          9.3418e-01, -2.5275e-01, -4.9055e-01,  9.5899e-01, -1.1113e+00,
          1.9128e+00],
        [-1.5182e+00,  8.0057e-01, -2.1892e-01,  1.0861e+00,  2.3597e-01,
         -8.8595e-01, -1.1603e+00,  7.6196e-01, -1.0109e+00,  7.4593e-01,
          8.6107e-01, -5.5711e-01, -5.1900e-01,  8.6750e-01, -1.3685e+00,
          1.8798e+00],
        [-1.5443e+00,  9.1712e-01, -1.6575e-01,  1.1037e+00,  2.2485e-01,
         -9.8380e-01, -1.2481e+00,  8.0236e-01, -1.1010e+00,  6.0796e-01,
          7.7311e-01, -4.4283e-01, -4.4822e-01,  9.0208e-01, -1.2373e+00,
          1.8402e+00],
        [-1.3154e+00,  1.4870e+00, -6.1769e-01,  1.0197e+00,  5.8131e-01,
         -1.0238e+00, -1.1357e+00,  2.2949e-01, -8.5536e-01,  9.1310e-01,
          1.3504e+00, -5.9708e-01, -9.2003e-01,  3.9683e-01, -9.8917e-01,
          1.4764e+00],
        [-1.6517e+00,  9.7498e-01, -1.3180e-01,  1.0676e+00,  2.1568e-01,
         -9.1460e-01, -1.3088e+00,  7.5019e-01, -1.1249e+00,  4.8131e-01,
          9.0143e-01, -3.3629e-01, -3.3243e-01,  9.6025e-01, -1.2668e+00,
          1.7159e+00],
        [-1.3835e+00,  1.4305e+00, -5.8510e-01,  1.0325e+00,  5.5190e-01,
         -9.9558e-01, -1.0551e+00,  3.2886e-01, -8.9234e-01,  9.2312e-01,
          1.2747e+00, -6.7144e-01, -1.0542e+00,  3.8049e-01, -8.4174e-01,
          1.5570e+00],
        [-1.4955e+00,  1.4418e+00, -5.6227e-01,  8.5998e-01,  4.1376e-01,
         -8.2677e-01, -1.0401e+00,  2.7711e-01, -9.0200e-01,  8.5802e-01,
          1.3044e+00, -6.0208e-01, -8.6193e-01,  8.1235e-01, -1.1699e+00,
          1.4931e+00],
        [-1.4327e+00,  1.5134e+00, -6.1812e-01,  1.1283e+00,  4.4760e-01,
         -8.7065e-01, -1.1055e+00,  3.9017e-01, -8.6601e-01,  8.6794e-01,
          1.2221e+00, -5.3318e-01, -1.2072e+00,  4.8410e-01, -8.3418e-01,
          1.4139e+00]], device='cuda:0')
tensor([[ 3.9076e-01,  9.0671e-01, -1.5497e+00, -5.7011e-01,  1.1695e+00,
         -1.5073e+00,  1.4741e+00,  5.4701e-01, -1.5942e-02,  5.2577e-01,
         -7.4714e-01, -1.1655e+00,  3.3088e-01,  1.5397e+00, -1.2256e+00,
         -1.0300e-01],
        [ 1.1611e+00, -9.7965e-01, -2.2005e+00, -5.7517e-01,  1.8851e+00,
         -3.8849e-01,  1.4647e+00, -8.3403e-02,  6.0598e-01, -6.2813e-01,
         -4.8911e-01,  4.1710e-01, -1.1660e-01,  6.7009e-01, -9.8289e-01,
          2.3986e-01],
        [ 3.1972e-01, -2.0162e-02, -1.2514e+00, -7.2439e-01,  6.9702e-01,
         -1.3951e+00,  7.5298e-01,  1.5234e+00,  1.1354e+00, -3.1063e-01,
         -1.0236e+00, -4.7973e-01,  3.6986e-01,  2.0568e+00, -1.0419e+00,
         -6.0833e-01],
        [ 7.0918e-01,  1.7339e-01, -1.9290e+00, -4.4314e-01,  4.0217e-01,
         -1.5602e+00,  8.3065e-01,  1.0217e+00,  1.4191e+00, -2.0877e-01,
         -7.1692e-01, -4.0909e-01,  4.0371e-01,  1.4514e+00, -1.4541e+00,
          3.0988e-01],
        [-8.7838e-01,  7.1330e-01, -1.5715e+00, -8.4975e-01,  2.7314e-01,
         -6.5189e-01,  1.7174e+00,  6.0212e-01,  1.6450e+00,  2.8939e-01,
         -1.2652e+00, -2.5064e-01, -3.6522e-01,  9.7262e-01, -1.2173e+00,
          8.3686e-01],
        [-4.8152e-01,  1.8826e-01, -1.4017e+00, -1.6063e+00,  2.8130e-01,
         -7.5934e-01,  3.9043e-01,  1.4716e+00,  1.6176e+00,  1.3815e-01,
         -1.5103e+00,  2.4213e-02,  2.5521e-01,  7.9727e-01, -8.0197e-01,
          1.3971e+00],
        [-1.0512e-02,  1.5097e-01, -1.4824e+00, -9.6737e-01,  2.4070e-01,
         -1.1530e+00,  2.1517e-01,  1.5045e+00,  1.6981e+00, -1.0010e-01,
         -1.4501e+00, -1.7565e-01,  6.1767e-01,  1.6223e+00, -1.0082e+00,
          2.9790e-01],
        [-5.2621e-02,  4.5938e-01, -1.5181e+00, -1.3763e+00,  1.7116e-01,
         -1.1288e+00,  4.1288e-01,  1.6647e+00,  1.3676e+00,  2.2833e-01,
         -1.4263e+00, -4.7933e-01,  6.3254e-01,  1.2901e+00, -8.5083e-01,
          6.0544e-01],
        [ 6.0146e-01,  7.8113e-02, -1.6293e+00, -4.5960e-01,  4.5721e-01,
         -1.5324e+00,  5.9393e-01,  1.4534e+00,  9.6021e-01, -4.8415e-03,
         -5.4244e-01,  7.4323e-02,  4.0964e-01,  1.8422e+00, -1.5448e+00,
         -7.5717e-01],
        [ 2.9817e-01, -7.6572e-02, -1.5878e+00, -4.3316e-01,  3.7632e-01,
         -1.2460e+00,  4.7571e-01,  1.2616e+00,  1.2629e+00, -3.0221e-03,
         -7.6883e-01,  1.3446e-01,  4.1268e-01,  2.1464e+00, -1.4131e+00,
         -8.3971e-01],
        [ 4.5695e-01, -2.2599e-01, -1.5885e+00, -6.3714e-01,  5.9917e-01,
         -1.3443e+00,  6.0889e-01,  1.3577e+00,  1.2716e+00, -1.5567e-01,
         -9.3001e-01,  4.2160e-03,  5.0984e-01,  1.9371e+00, -1.2276e+00,
         -6.3625e-01],
        [ 3.9564e-01,  1.1244e-03, -1.6489e+00, -7.7138e-01,  3.2960e-01,
         -1.2948e+00,  3.8706e-01,  1.6107e+00,  1.4222e+00, -2.9628e-01,
         -1.1305e+00, -2.8059e-01,  7.4656e-01,  1.6967e+00, -1.0391e+00,
         -1.2794e-01],
        [ 6.4467e-01, -3.9291e-02, -1.9578e+00, -8.5074e-02,  2.6628e-01,
         -1.5553e+00,  5.7919e-01,  1.1563e+00,  1.4882e+00, -5.1842e-01,
         -6.9159e-01, -3.9178e-01,  4.3912e-01,  1.6726e+00, -1.1988e+00,
          1.9180e-01],
        [ 3.1836e-01, -1.8503e-02, -1.5156e+00, -9.7231e-01,  4.4707e-01,
         -1.2219e+00,  6.9039e-01,  1.3823e+00,  1.2907e+00, -2.4863e-02,
         -1.2090e+00, -3.4025e-01,  6.9895e-01,  1.9078e+00, -1.0134e+00,
         -4.1965e-01],
        [ 4.4730e-01, -9.7724e-02, -1.6347e+00, -8.3806e-01,  3.5409e-01,
         -1.3283e+00,  4.6669e-01,  1.6088e+00,  1.3006e+00, -1.6158e-01,
         -1.0709e+00, -1.4834e-01,  7.4056e-01,  1.7469e+00, -1.0620e+00,
         -3.2329e-01],
        [-1.3537e+00,  1.4461e+00, -6.8085e-01, -5.3858e-01, -2.9917e-02,
         -1.2054e+00,  4.1052e-01,  8.7689e-01,  1.0569e+00,  7.9388e-01,
         -8.6260e-02, -1.3434e+00,  4.6485e-01,  1.7051e+00, -1.4988e+00,
         -1.7314e-02],
        [ 5.7950e-01,  1.8112e-02, -1.8837e+00, -5.7459e-01,  2.9925e-01,
         -1.3863e+00,  4.4498e-01,  1.3611e+00,  1.3469e+00, -1.4156e-01,
         -8.3608e-01, -1.1451e-01,  6.1719e-01,  1.6968e+00, -1.3836e+00,
         -4.3569e-02],
        [-1.3033e+00,  9.7230e-01, -2.8123e-01, -3.3542e-01, -3.4886e-01,
         -7.5224e-01,  5.5547e-01,  5.5182e-01,  1.2324e+00,  5.8562e-01,
         -3.0369e-01, -1.3808e+00,  2.4123e-01,  2.4173e+00, -1.3659e+00,
         -4.8470e-01],
        [-1.3933e+00, -3.2850e-02, -2.5322e-01, -1.8226e+00,  5.5914e-01,
          3.5627e-01,  4.4769e-02,  1.2682e+00,  1.3404e+00,  8.2291e-01,
         -1.8737e+00, -9.5503e-01,  2.3363e-01,  1.0264e+00, -9.8812e-02,
          7.7776e-01],
        [-1.2755e+00,  1.2185e+00,  9.7057e-02, -6.1771e-01,  7.3758e-02,
         -3.6380e-01,  1.0143e+00,  6.5155e-01,  8.5460e-01,  4.9211e-01,
         -1.0199e+00, -1.9063e+00, -2.4104e-01,  2.0640e+00, -9.6600e-01,
         -7.5738e-02]], device='cuda:0')
Analyzing molecule stability...
Validity over 9216 molecules: 0.03%
Number of connected components of 9216 molecules: min:15.00 mean:18.67 max:23.00
Relaxed validity over 9216 molecules: 0.04%
Uniqueness over 4 valid molecules: 100.00%
Novelty over 4 unique valid molecules: 100.00%
{'Validity': 0.0003255208333333333, 'Relaxed Validity': 0.00043402777777777775, 'Uniqueness': 1.0, 'Novelty': 1.0, 'nc_max': 23, 'nc_mu': 18.666666666666668, 'FCD_Test': 53.37472418430574, 'NSPDK_MMD': 0.45436271422877156}
Graph with 4 nodes and 3 edges
Graph with 2 nodes and 1 edges
Graph with 3 nodes and 2 edges
Graph with 1 nodes and 0 edges
val: {'epoch': 0, 'time_epoch': 3711.07766, 'loss': 0.90398979, 'lr': 0, 'params': 7543411, 'time_iter': 412.34196, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0}
...computing epoch stats took: 0.03s
tensor([[-1.8122,  1.2001, -0.3126,  1.1395,  0.1101, -0.7886, -1.0363,  1.0279,
         -1.0468,  0.8072,  1.1526, -0.5032, -0.9449,  0.4201, -0.8341,  1.4212],
        [-1.8871,  1.2599, -0.0225,  1.3806, -0.0673, -0.6449, -1.1201,  0.5274,
         -1.3046,  0.7749,  1.4276, -0.5108, -0.3121,  0.4675, -1.0023,  1.0337],
        [-1.7983,  1.1444, -0.0855,  1.2164,  0.0492, -0.7686, -1.2372,  0.6246,
         -1.3363,  0.8229,  1.2135, -0.3986, -0.3073,  0.6933, -1.0933,  1.2608],
        [-1.9858,  1.2827,  0.0834,  1.3369, -0.1890, -0.6627, -1.1881,  0.5723,
         -1.0886,  0.7946,  1.0575, -0.5968, -0.4434,  0.4913, -0.8566,  1.3925],
        [-1.8111,  0.9743, -0.1928,  1.2348,  0.0214, -0.8278, -1.0633,  1.1894,
         -1.1974,  0.7632,  1.0128, -0.4362, -0.7343,  0.5401, -0.9144,  1.4414],
        [-1.9189,  1.2148, -0.0475,  1.3567, -0.0207, -0.7750, -1.1641,  0.6654,
         -1.2281,  0.7517,  1.3011, -0.5149, -0.5489,  0.5091, -0.7700,  1.1892],
        [-1.8909,  1.0655,  0.1231,  1.3075, -0.0137, -0.8029, -1.1359,  0.7767,
         -1.1364,  0.7291,  1.0210, -0.6550, -0.5303,  0.5416, -0.9195,  1.5201],
        [-1.8738,  0.9639,  0.2396,  1.2573, -0.0631, -0.7789, -1.3562,  0.6473,
         -1.1112,  0.3368,  0.8686, -0.4050, -0.3107,  0.8741, -0.9992,  1.7105],
        [-2.0327,  1.5185, -0.1906,  1.0079, -0.3065, -0.5155, -1.0671,  0.9630,
         -0.8158,  0.7516,  0.9514, -0.5707, -0.9046,  0.5308, -0.7298,  1.4101],
        [-1.8329,  1.0728, -0.1692,  1.2111, -0.0325, -0.7734, -1.0316,  1.0108,
         -1.1149,  0.8845,  1.0661, -0.6486, -0.9475,  0.6295, -0.7115,  1.3874],
        [-1.9500,  1.4098, -0.2176,  1.1571,  0.0413, -0.6945, -1.1976,  0.7668,
         -1.0396,  0.5959,  1.2815, -0.4094, -0.6696,  0.4994, -0.8550,  1.2813],
        [-1.6964,  1.2059, -0.2417,  1.1493,  0.2959, -0.8444, -1.1883,  0.3694,
         -1.3360,  1.0558,  1.3773, -0.5681, -0.3162,  0.5480, -1.0010,  1.1904],
        [-1.8295,  1.2840, -0.0577,  1.4238, -0.0970, -0.9165, -1.2366,  0.6458,
         -1.1915,  0.8551,  1.2781, -0.4590, -0.3769,  0.2872, -0.7747,  1.1652],
        [-1.9882,  1.2430,  0.2251,  1.3079, -0.0208, -0.7847, -1.2824,  0.5875,
         -1.1112,  0.5339,  1.0368, -0.5301, -0.3060,  0.5523, -0.8966,  1.4336],
        [-2.0712,  1.2009,  0.3051,  1.2433, -0.2779, -0.5486, -1.2859,  0.7292,
         -1.0058,  0.4071,  0.8468, -0.4749, -0.3063,  0.7413, -1.0120,  1.5090],
        [-1.8534,  1.2227, -0.1862,  1.2448, -0.0438, -0.7791, -1.1367,  1.0256,
         -1.0780,  0.7153,  0.9895, -0.4304, -0.7357,  0.5132, -0.9058,  1.4381],
        [-1.9486,  1.2619,  0.0829,  1.2800, -0.0249, -0.8326, -1.3023,  0.7037,
         -1.0049,  0.5637,  0.9787, -0.4604, -0.3160,  0.5659, -1.0104,  1.4632],
        [-1.8989,  1.2030,  0.2195,  1.2766, -0.0733, -0.6587, -1.3035,  0.5695,
         -1.1638,  0.5246,  0.9792, -0.4682, -0.1913,  0.6500, -1.1519,  1.4871],
        [-1.8231,  1.1986, -0.4787,  1.1232, -0.0671, -0.5644, -1.0206,  0.9236,
         -1.1594,  0.9230,  1.0811, -0.4270, -0.7603,  0.7202, -1.0244,  1.3553],
        [-1.9287,  1.0576, -0.2352,  1.1690, -0.1684, -0.4322, -1.1125,  0.7949,
         -1.0571,  0.7466,  1.0473, -0.5332, -0.6687,  0.8591, -1.0551,  1.5164]],
       device='cuda:0')
tensor([[ 4.6352e-01,  1.2309e+00, -1.1771e+00, -1.5383e-01,  2.4468e-01,
         -1.6599e+00,  5.6865e-01,  8.9647e-01,  5.2847e-01,  4.5266e-01,
         -4.9096e-01, -1.4905e+00,  1.9851e-02,  1.9131e+00, -1.5112e+00,
          1.6523e-01],
        [-6.8964e-01, -1.6953e+00, -1.2381e+00, -1.2281e+00,  1.0773e+00,
          3.9938e-01,  3.0587e-01,  2.6085e-01,  4.0555e-01,  1.0929e+00,
         -1.5240e+00,  1.9887e+00, -1.8922e-03,  2.9919e-01, -7.9618e-02,
          6.2685e-01],
        [-5.5757e-01,  8.0574e-01, -1.7878e+00,  5.2124e-01, -2.6192e-01,
         -1.8517e+00, -3.8421e-01,  6.2051e-01,  1.1838e+00,  2.7388e-01,
         -1.4057e-02,  1.1906e-01,  6.2532e-01,  1.3503e+00, -1.6683e+00,
          1.0258e+00],
        [-5.9894e-01,  2.2092e-01, -2.1493e+00, -9.6028e-01,  2.9084e-01,
         -1.2447e+00,  1.0155e+00,  4.8755e-01,  1.1548e+00,  1.3709e+00,
         -9.3643e-01,  3.1241e-01,  5.8445e-01,  5.2707e-01, -1.1180e+00,
          1.0433e+00],
        [-1.3883e-01,  2.7953e-01, -1.8497e+00, -6.2253e-01,  2.9201e-01,
         -1.4386e+00, -6.2710e-02,  1.5930e+00,  1.2076e+00, -5.9519e-02,
         -1.2633e+00, -9.0352e-02,  7.3691e-01,  1.4577e+00, -9.3207e-01,
          8.9079e-01],
        [-7.7022e-01,  6.2874e-01, -1.6732e+00, -5.7481e-01,  2.8026e-01,
         -1.2076e+00, -6.2900e-01,  1.6918e+00,  1.2540e+00,  4.2517e-01,
         -1.4120e+00,  6.0161e-01,  6.0568e-01,  4.4331e-01, -9.1736e-01,
          1.2536e+00],
        [-4.4057e-01,  6.1461e-01, -1.8329e+00, -3.2236e-01, -2.9710e-02,
         -1.4843e+00, -4.6195e-01,  1.6862e+00,  1.1603e+00, -6.0643e-02,
         -1.2086e+00,  4.9440e-01,  7.3079e-01,  1.1386e+00, -9.4509e-01,
          9.6120e-01],
        [ 6.1296e-01,  4.6735e-01, -1.7156e+00, -5.2677e-01,  3.6369e-01,
         -1.4229e+00,  3.6582e-01,  1.3552e+00,  1.5530e+00, -2.1998e-01,
         -9.0938e-01, -7.0825e-01,  5.9403e-01,  1.4278e+00, -1.3639e+00,
          1.2691e-01],
        [-7.9230e-01,  9.6548e-01, -1.5239e+00, -5.6143e-01,  1.0226e-01,
         -1.3230e+00, -3.8754e-01,  1.8919e+00,  1.0319e+00, -1.2543e-02,
         -1.3939e+00,  4.6505e-01,  6.9578e-01,  1.1804e+00, -9.2810e-01,
          5.8988e-01],
        [-4.8442e-01,  9.4720e-01, -1.8520e+00, -2.7527e-01,  1.0334e-01,
         -1.5078e+00, -3.6996e-01,  1.7377e+00,  9.4478e-01, -2.0504e-02,
         -1.0318e+00,  8.0997e-01,  5.6202e-01,  1.0251e+00, -1.2343e+00,
          6.4594e-01],
        [-4.8336e-01, -2.7557e-01, -1.2451e+00, -1.4650e+00,  2.2246e-01,
         -4.1902e-01, -2.6249e-01,  1.7264e+00,  9.8740e-01,  5.4214e-01,
         -2.2223e+00,  6.1020e-01,  4.1804e-01,  5.4230e-01,  3.2359e-02,
          1.2916e+00],
        [-1.0029e+00, -8.4951e-01, -1.7705e+00, -1.0395e+00,  5.5015e-01,
         -5.1170e-01,  8.7847e-01,  3.3791e-01,  8.6674e-01,  1.0440e+00,
         -1.4736e+00,  8.3656e-01,  3.3807e-01,  3.5577e-01, -4.2347e-01,
          1.8635e+00],
        [-9.5084e-01,  4.2799e-01, -1.4160e+00, -7.0530e-01,  3.0600e-01,
         -1.0700e+00, -6.1804e-01,  1.5535e+00,  1.3401e+00,  5.1833e-01,
         -1.6452e+00,  2.0887e-01,  4.6925e-01,  3.0353e-01, -4.6346e-01,
          1.7414e+00],
        [-6.3379e-01,  1.8328e-01, -1.7137e+00, -9.8995e-01,  6.5935e-01,
         -8.1310e-01, -3.6233e-01,  1.7824e+00,  1.1370e+00,  2.8021e-01,
         -1.4624e+00,  3.3725e-01,  4.4235e-01,  1.7403e-01, -7.1707e-01,
          1.6965e+00],
        [-6.4407e-02,  4.1045e-01, -1.5944e+00, -6.4599e-01,  5.9874e-02,
         -1.1617e+00, -4.4707e-01,  1.8278e+00,  1.5747e+00, -1.5980e-01,
         -1.5015e+00, -7.3413e-02,  7.2480e-01,  1.3846e+00, -7.6266e-01,
          4.2864e-01],
        [-5.7613e-01,  4.9578e-01, -1.5196e+00, -1.0319e+00, -8.6742e-02,
         -1.1852e+00, -2.7597e-01,  1.6946e+00,  1.2876e+00,  2.3932e-02,
         -1.6672e+00,  1.2730e-01,  8.5100e-01,  9.5339e-01, -3.1541e-01,
          1.2248e+00],
        [-5.7189e-01, -4.5438e-01, -1.5738e+00, -1.1040e+00,  4.0775e-01,
         -6.9800e-01,  1.6505e-01,  1.4623e+00,  7.4157e-01,  6.4065e-01,
         -1.9105e+00,  6.5440e-01,  1.3333e-01,  5.2531e-01, -2.9649e-01,
          1.8787e+00],
        [-2.9383e-01,  4.2639e-01, -1.6001e+00, -2.2286e-01, -1.8313e-01,
         -1.6975e+00, -3.2154e-01,  1.1690e+00,  1.7398e+00,  7.7519e-02,
         -8.5868e-01, -1.5953e-01,  7.1614e-01,  1.4079e+00, -1.2010e+00,
          1.0014e+00],
        [-6.8964e-01, -1.6953e+00, -1.2381e+00, -1.2281e+00,  1.0773e+00,
          3.9938e-01,  3.0587e-01,  2.6085e-01,  4.0555e-01,  1.0929e+00,
         -1.5240e+00,  1.9887e+00, -1.8922e-03,  2.9919e-01, -7.9618e-02,
          6.2685e-01],
        [ 1.6993e-01,  3.1897e-01, -9.3668e-01, -1.7091e+00,  1.1807e+00,
         -5.8674e-01,  6.7479e-01,  1.9295e+00,  1.0889e+00, -3.9458e-01,
         -1.4129e+00, -1.1543e+00,  4.2579e-01,  1.0460e+00, -2.8377e-01,
         -3.5667e-01]], device='cuda:0')
Analyzing molecule stability...
Validity over 9216 molecules: 0.01%
Number of connected components of 9216 molecules: min:22.00 mean:22.00 max:22.00
Relaxed validity over 9216 molecules: 0.01%
Uniqueness over 1 valid molecules: 100.00%
Novelty over 1 unique valid molecules: 100.00%
{'Validity': 0.00010850694444444444, 'Relaxed Validity': 0.00010850694444444444, 'Uniqueness': 1.0, 'Novelty': 1.0, 'nc_max': 22, 'nc_mu': 22.0, 'FCD_Test': nan, 'NSPDK_MMD': 1.18992385751254}
Graph with 2 nodes and 1 edges
test: {'epoch': 0, 'time_epoch': 3623.5065, 'loss': 0.90018096, 'lr': 0, 'params': 7543411, 'time_iter': 402.61183, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0}
...computing epoch stats took: 0.04s
> Epoch 0: took 13689.2s (avg 13689.2s) | Best so far: epoch 0	train_loss: 21.4984 train_mae: 0.0000	val_loss: 0.9040 val_mae: 0.0000	test_loss: 0.9002 test_mae: 0.0000
train: {'epoch': 1, 'time_epoch': 3211.49743, 'eta': 3373708.83993, 'eta_hours': 937.14134, 'loss': 11.28126573, 'lr': 1e-05, 'params': 7543411, 'time_iter': 1.15813, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 10.134864441039307, 'loss_edge': 1.1464012982816565}
...computing epoch stats took: 1.07s
train: {'epoch': 2, 'time_epoch': 3218.29799, 'eta': 3316433.27762, 'eta_hours': 921.23147, 'loss': 10.05545154, 'lr': 2e-05, 'params': 7543411, 'time_iter': 1.16058, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 9.22910461809317, 'loss_edge': 0.8263469247759974}
...computing epoch stats took: 1.25s
train: {'epoch': 3, 'time_epoch': 3248.22422, 'eta': 3293637.97926, 'eta_hours': 914.89944, 'loss': 9.25828965, 'lr': 3e-05, 'params': 7543411, 'time_iter': 1.17138, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 8.597739262910043, 'loss_edge': 0.6605503932463314}
train: {'epoch': 4, 'time_epoch': 3207.95839, 'eta': 3270648.61071, 'eta_hours': 908.5135, 'loss': 8.31241381, 'lr': 4e-05, 'params': 7543411, 'time_iter': 1.15685, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 7.833659651714273, 'loss_edge': 0.4787541572140288}
train: {'epoch': 5, 'time_epoch': 3211.08251, 'eta': 3254770.60746, 'eta_hours': 904.10295, 'loss': 7.24753981, 'lr': 5e-05, 'params': 7543411, 'time_iter': 1.15798, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 6.943421774312171, 'loss_edge': 0.30411803118572117}
train: {'epoch': 6, 'time_epoch': 3174.10719, 'eta': 3237266.5113, 'eta_hours': 899.2407, 'loss': 6.13700223, 'lr': 6e-05, 'params': 7543411, 'time_iter': 1.14465, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 5.970025562522668, 'loss_edge': 0.1669766683153312}
train: {'epoch': 7, 'time_epoch': 3172.43938, 'eta': 3223138.10412, 'eta_hours': 895.31614, 'loss': 5.04784873, 'lr': 7e-05, 'params': 7543411, 'time_iter': 1.14405, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 4.959960072233468, 'loss_edge': 0.08788865813156226}
train: {'epoch': 8, 'time_epoch': 3592.99111, 'eta': 3257751.775, 'eta_hours': 904.93105, 'loss': 4.02144628, 'lr': 8e-05, 'params': 7543411, 'time_iter': 1.29571, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 3.964965524628261, 'loss_edge': 0.056480751089344715}
train: {'epoch': 9, 'time_epoch': 4066.86696, 'eta': 3331637.82272, 'eta_hours': 925.45495, 'loss': 3.08233147, 'lr': 9e-05, 'params': 7543411, 'time_iter': 1.46659, 'mae': 0.0, 'r2': 1.0, 'spearmanr': nan, 'mse': 0.0, 'rmse': 0.0, 'loss_node': 3.0358319489388967, 'loss_edge': 0.04649951730036712}
